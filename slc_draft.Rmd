---
title: "insurance model"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Task: build multiple linear regression and binary logistic regression models on the training data to predict the probability that a person will crash their car and also the amount of money it will cost if the person does crash their car. Using the training data set, build at least two different multiple linear regression models and three different binary logistic regression models, using different variables (or the same variables with different transformations).

```{r echo = FALSE, message = FALSE}
library(tidyverse)
library(janitor)
library(magrittr)
library(flextable)
library(dlookr) # eda
library(ggpubr) # creates a wrapper for plotting a list
library(viridis)
library(broom) # creates a tidy data frame from statistical test results
library(car) # VIF
library(lmtest) #partial likelihood test for model fit comparisons
library(caret) # conf matrix
library(car) # marginal model plot
library(pROC) #roc curve
```

Import the dataset

Findngs and Response: 

1. There are no duplicate rows to worry about here.


```{r}
raw <- read_csv('insurance_training_data.csv')

# initial clean of col names

raw%<>%clean_names

# remove any empty rows and cols

raw%<>%remove_empty(c("rows", "cols"))

# assess presence of duplicates

get_dupes(raw)

# basic characterization

str(raw)

#clean extraneous symbols from char col values and convert to numeric as appropriate

df<-raw%>%
    mutate_if(is_character, str_replace_all, '\\$|,|z_|<', '')%>%
    mutate_at(c(8,10,17,21), as.numeric)%>%
    mutate_at(c(2), as.factor)

# round numeric columns

df%<>%mutate_if(is.numeric, round)

# identify unique values in our character cols

id_distinct <- df%>%
    select(where(is_character))%>%
    map(~str_c(unique(.x),collapse = ",")) %>% 
    bind_rows() %>% 
    gather(key = col_name, value = col_unique)

# Update col values for clarity and brevity

df%<>%
    mutate_if(is_character, str_replace_all, "No|no",'N')%>%
    mutate_if(is_character, str_replace_all, "Yes|yes",'Y')%>%
    mutate_if(is_character, str_replace_all, "Highly Urban/ Urban",'Urban')%>%
    mutate_if(is_character, str_replace_all, "Highly Rural/ Rural",'Rural')

# convert character cols to factor and set level for education col

df %<>%mutate_if(is_character, ~(factor(.)))

df$education<-factor(df$education, levels=c("High School", "Bachelors", "Masters", "PhD"))
  
str(df)
```
Assess missing data - dlookr package

Recommend: impute missing numerical data 

```{r fig.height = 8, fig.width = 15}

#basic missing table
    
df%>%
    diagnose()%>%
    select(-unique_count, -unique_rate)%>%
    filter(missing_count>0)%>%
    arrange(desc(missing_count))%>%
    flextable()

# missing plots 

df%>%plot_na_pareto(only_na = TRUE) # only plots vars with missing
df%>%plot_na_intersect(only_na = TRUE)

```
Assess other data characteristics - tables

Findings:

Skewed Data - target_amt, kidsdriv, homekids, oldclaim, clm_freq, mvr_pts, yoj, income

Percent Outliers (>5%) - target_amt, kidsdriv, homekids, oldclaim, yoj

Errors (neg vals) - car_age (1 obs)

Recommend:

- convert neg value in car_age to NA

Possible transformations

- travtime (sqrt), bluebook(sqrt), old-claim(log), homeval(log), income(sqrt)

```{r}
#identify highly skewed data

df%>%find_skewness(index=FALSE, thres=TRUE) #this is good

#identify outliers

df%>%find_outliers(index=FALSE, rate=TRUE) # this is good

# assess normality - Shapiro Wilke

df%>%normality()

df%>%plot_normality()	

# other diagnostics

df%>%
    diagnose_numeric()%>%
    select(variables, min, mean, median, max, zero, minus)%>%
    flextable(theme_fun = theme_booktabs())

df%>%
    diagnose_category()%>%
    flextable(theme_fun = theme_booktabs())
```

Correct obvious data errors

```{r}

df$car_age[df$car_age < 0] <- NA


```

Pairwise comparisons

Findings:

1. Only income and homeval corr at >0.5 or <-0.5

```{r fig.height=20, fig.width+20}

#assess covariance 

df%>%
    select(!index)%>%
    correlate()%>%
    filter(coef_corr > .5 | coef_corr < -.5)

df%>%plot_correlate()  
```

Impute numerical values 

NOte: imputation method determined through iterative trials

```{r}

#impute numerical vars using dlookr, methods have been preselected based on initial plots.

#car_age

plot(imputate_na(df, car_age, target_flag, method = "mice", seed = 999))+
  theme_minimal()+ theme(legend.position = "top")

car_age<-imputate_na(df, car_age, target_flag, method = "mice", seed = 999)

summary(car_age)


#home_val

plot(imputate_na(df, home_val, target_flag, method = "rpart"))+
  theme_minimal()+
  theme(legend.position = "top")

home_val<-imputate_na(df, home_val, target_flag, method = "rpart")

summary(home_val)

#yoj

plot(imputate_na(df, yoj, target_flag, method = "rpart"))+
  theme_minimal()+
  theme(legend.position = "top")

yoj<- imputate_na(df, yoj, target_flag, method = "rpart")

summary(yoj)

# income

plot(imputate_na(df, income, target_flag, method = "rpart"))+
  theme_minimal()+
  theme(legend.position = "top")

income<-imputate_na(df, income, target_flag, method = "rpart")

summary(income)

#age

plot(imputate_na(df, age, method = "rpart"))+  #any of the tests work here
  theme_minimal()+
  theme(legend.position = "top")

age<-imputate_na(df, age, method = "rpart")

summary(age)

temp<-cbind(car_age, home_val, yoj, income, age)
temp%<>%as.data.frame(temp)

df%<>%select(!c(car_age, home_val, yoj, income, age))%>%
    cbind(temp)

df%<>%mutate_if(is.numeric, round)

#write.csv(df, 'C://Users//seanc//Documents//Data_Science//CUNY//621_BusinessAnalytics//insurance_model//df_clean.csv', row.names=FALSE)



```
Impute Categorical Variables - job

```{r}

job<-imputate_na(df, job, method = "mice", seed = 999)

plot(job)


# combine into new dfb

df<-df%>%select(!job)
df<-cbind(df,job)
df$job<-factor(df$job)

```
Save csv with imputed values 

```{r}
#write.csv(df, 'C://Users//seanc//Documents//Data_Science//CUNY//621_BusinessAnalytics//insurance_model//df_clean_imputed.csv', row.names=FALSE)
```


Assess Outliers

Findings:

1. Outliers (>5%) - target_amt, kidsdriv, homekids, oldclaim, yoj

2. High Zero Freq - target_amt, kidsdriv, homekids, oldclaim, mvr_pts, car_age, 
    home_val (why?)

Recommend: 

- create  2-level factor for kids_drive, home_kids
- create 2 or 3 -level factor for tiff(?)
- respond to zero inflation for target_age, old_claim and car age (?)

```{r}
diagnose_outlier(df) %>% flextable()

df %>% 
    select(find_outliers(df, index = FALSE)) %>% 
    plot_outlier()
```


# Review categorical vars in relation to target

Findings:

1. Obv. Patterns - parent1, mstatus, education, job, car use, car type, revoked, urbanicity

```{r fig.height=5, fig.width=8}

df %>% 
  target_by(target_flag) %>%
    relate(parent1) %>%
    plot()

df %>% 
  target_by(target_flag) %>%      
  relate(mstatus) %>% 
  plot()

df %>% 
  target_by(target_flag) %>%      
  relate(sex) %>% 
  plot()

df %>% 
  target_by(target_flag) %>%      
  relate(education) %>%
  plot()
  

df %>% 
  target_by(target_flag) %>%      
  relate(job) %>% 
  plot()

df %>% 
  target_by(target_flag) %>%      
  relate(car_use) %>% 
  plot()

df %>% 
  target_by(target_flag) %>%      
  relate(car_type) %>% 
  plot()

df %>% 
  target_by(target_flag) %>%      
  relate(red_car) %>% 
  plot()

df %>% 
  target_by(target_flag) %>%      
  relate(revoked) %>% 
  plot()

df %>% 
  target_by(target_flag) %>%      
  relate(urbanicity) %>% 
  plot()



```
Plot numerical variables

```{r fig.height=15, fig.width=25}

num_box<-select_if(df, is.numeric)
num_box<-cbind(df$target_flag, num_box)%>%
    rename(target_flag = 'df$target_flag')


response = names(num_box)[1] #target_flag
response = purrr::set_names(response)

explain <- names(num_box)[3:16] #explanatory variables
explain = purrr::set_names(explain)

box_fun = function(x) {
    ggplot(num_box, aes_string(x = x, y = 'target_flag') ) +
    geom_boxplot(aes(fill = target_flag, alpha = 0.4), outlier.color =
    'red', show.legend = FALSE)+
    scale_fill_viridis(discrete = TRUE, option = "E")+
    coord_flip()+
    theme_classic()
    
}

b_plots<-map(explain, ~box_fun(.x)) #creates a list of plots

ggarrange(plotlist=b_plots, height = .5, ncol = 3)
```
Feature engineering - factor and levels

```{r}


df%<>%
    select(!c(target_amt))

# change kidsdrive to categorical
    
df%<>%
    mutate(kidsdriv = case_when(kidsdriv == 0 ~ 'N'
         ,TRUE  ~ 'Y'))

# change job into blue collar and professional levels

df%<>%
    mutate(job = case_when(job == 'Blue Collar' ~ 'Blue Collar',
                           job != 'Blue Collar' ~ 'Professional', 
                           TRUE ~ as.character(NA)))

df%<>%mutate(job = na_if(job, "NA"))

#change chars to factors and level education

df %<>%mutate_if(is_character, ~(factor(.)))%>%select(!index)

df$education<-factor(df$education, levels=c("High School", "Bachelors", "Masters", "PhD"))


```
Base logistic model 

- based on prior work we remove red_car and age

```{r}

model_df<-df%>%select(!c(red_car, age))

base_model <- glm(target_flag ~ .,family='binomial', model_df)

summary(base_model)
```
#check assumptions - linearity

Findings: yoj and travtime may not be linear with regard to logit

```{r}

#include probabilities and logit to df

model_df_vars<-model_df

model_df_vars$predicted<-predict(base_model, type='response')

model_df_vars%<>%
    mutate(logit = log(predicted/(1-predicted)))

# check linearity btwn numerical predictors and logit

with(model_df_vars, scatter.smooth(travtime, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(model_df_vars, scatter.smooth(tif, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(model_df_vars, scatter.smooth(mvr_pts, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(model_df_vars, scatter.smooth(home_val, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(model_df_vars, scatter.smooth(yoj, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(model_df_vars, scatter.smooth(income, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(model_df_vars, scatter.smooth(homekids, logit, lpars = list(col = "red", lwd = 3, lty = 3)))

```

Review possible tranformations based on distributions of numerical predictors

findings: sqrt travtime and log yoj

```{r}
plot_normality(logistic2)

```



#Evaluate outliers and infuential obs

computes the standardized residuals (.std.resid) and the Cookâ€™s distance (.cooksd) using the R function augment() [broom package]

See: http://www.sthda.com/english/articles/36-classification-methods-essentials/148-logistic-regression-assumptions-and-diagnostics-in-r/

Findings: we have one influential obs #5101
```{r}

# Extract model results

base_model$data <- augment(base_model) %>% 
  mutate(index = 1:n()) 

#top 3 largest values

base_model$data %>% top_n(3, .cooksd)

plot(base_model, which = 4, id.n = 3)

#plot std residuals

ggplot(base_model$data, aes(index, .std.resid)) + 
  geom_point(aes(color = target_flag), alpha = .5) +
  theme_bw()

#Filter potential influential data points with abs(.std.res) > 3

base_model$data %>% 
  filter(abs(.std.resid) > 3)
```
# Assess multicollinearity

Findings: no problems with collinearity

```{r}
car::vif(base_model)
```
#rerun base model without the influential obs 

```{r}

model_df<-model_df%>%slice(-5101) # remove influential obs

base_model <- glm(target_flag ~ .,family='binomial', model_df) 

summary(base_model)
```
# Check residuals

```{r fig.height=10, fig.width=15}

res_chk<-model_df%>%na.omit()%>%mutate('residuals' = residuals(base_model), linpred = predict(base_model))

gdf<- group_by(res_chk, cut(linpred, breaks=unique(quantile(linpred, (1:100)/101))))

diagdf<-summarize(gdf, residuals=mean(residuals), linpred = mean(linpred))

plot(residuals~linpred, diagdf, xlab = "linear predictor")


```
#check marginal model

Findings: we have issues with trav_time, yoj, and possibly mvr points. 

```{r fig.align='center', fig.height=10,fig.width=10, message = FALSE}
marginals<-mmps(base_model,main=NULL)
```

Transformations

- limit travtime to remove extreme distances (this may not be defensible)
- remove yoj
- sqrt: travtime, home_val, income, clm_freq. Note that the model would not converge using log transformations. 

```{r}

trans_df<-model_df%>%filter(travtime <100)

trans_model <- glm(target_flag~kidsdriv+parent1+mstatus+sex+education+job+sqrt(travtime)+
                  car_use+tif+sqrt(clm_freq)+revoked+mvr_pts+urbanicity+sqrt(home_val)+sqrt(income),family='binomial',trans_df)
summary(trans_model)
```

Plot marginal contributions with the transformed variable model

```{r fig.align='center', fig.height=10,fig.width=10, message = FALSE}

marg<-mmps(trans_model,main=NULL)
```

```{r }
#include probabilities and logit to df

trans_df_var<-trans_df#%>%na.omit()

trans_df_var$predicted<-predict(trans_model, type='response')

trans_df_var%<>%
    mutate(logit = log(predicted/(1-predicted)))

# check linearity btwn numerical predictors and logit

with(trans_df_var, scatter.smooth(travtime, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(trans_df_var, scatter.smooth(tif, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(trans_df_var, scatter.smooth(mvr_pts, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(trans_df_var, scatter.smooth(home_val, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(trans_df_var, scatter.smooth(income, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(trans_df_var, scatter.smooth(homekids, logit, lpars = list(col = "red", lwd = 3, lty = 3)))


```

#check residuals for transformed var model

Note - the plot includes raw resids. Would studentized residuals be more appropriate?

```{r}
trans_resid<-trans_df%>%na.omit()%>%mutate('residuals' = residuals(trans_model), linpred = predict(trans_model))

gdf<- group_by(trans_resid, cut(linpred, breaks=unique(quantile(linpred, (1:100)/101))))

diagdf<-summarize(gdf, residuals=mean(residuals), linpred = mean(linpred))

plot(residuals~linpred, diagdf, xlab = "linear predictor")

```





#other fit assessments - confusion matrix, ROC, AUC

prediction threshold can be varied depending on desired cost:benefit for insurer
```{r}

# create col for class category (0,1) based on predicted probabilities 

cm_df<-trans_df_var%>%
    mutate(predicted_obs = case_when(
        predicted >= 0.5 ~ 1,
        predicted < 0.5 ~0))

#ensure the obs and predicted are factors with same levels

cm_df$predicted_obs<-as.factor(trans_df_var$predicted_obs)
cm_df$target_flag<-as.factor(trans_df_var$target_flag)

#confusion matrix

confusionMatrix(data = trans_df_var$predicted_obs, reference = trans_df_var$target_flag)

#roc curve with AUC

par(pty='s')

proc<- roc(response=trans_df_var$target_flag, predictor=trans_df_var$predicted, plot=TRUE, legacy.axes=TRUE, auc.polygon=TRUE, col='blue', main = 'PROC ROC Curve', max.auc.polygon=TRUE, print.auc=TRUE)

```

Assess goodness of fit between base model and transformed var model
```{r}


#compare difference for base_model and base_modelb coefficients (no strong diff) 

c_base<-coef(base_model)%>%as.list() # convert to list to remove yoj

c_base<-c_base%>% purrr::list_modify("yoj" = NULL)

c_base%<>%as.double() #convert back to double-precision vector

delta.coef<-abs((coef(trans_df_var)-c_base)/c_base)

round(delta.coef,3)

#partial likelihood ratio test to compare fit of the two models (no diff)

lrtest(base_model, trans_df_var)

#Anova to compare fit (none found)

anova(base_model, trans_df_var, test='Chisq')

```

