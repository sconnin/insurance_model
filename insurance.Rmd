---
title: "Insurance Classification and Prediction"
date: "1/11/22"
author: "Sean Connin"
output:
  html_document: 
    toc: true
    toc-title: ""
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: false
    theme: united
    highlight: tango
  pdf_document: default
---

```{r setup, include=FALSE}

# set global options

knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message = FALSE)

```
##Project Description##

The purpose of this work is to estimate the probability that an individual (seeking auto insurance) will be in an accident and then to forecast the potential cost of an ensuing claim. A synthetic data set of ~ 8000 observations will be used to construct predictive models for this purpose. The use of synthetic data permits model development absent proprietary information, while also providing a heuristic for quantitative reasoning.

The dataset includes two response variables. The first, 'target_flag', is scored as either a 1 or 0 indicating that an individual has or has not been in an accident, respectively. On this basis, a score of 1 is considered a 'positive' result and 0 a 'negative' result. The second response variable, 'target_amt' refers to the amount paid on a individual's claim given an accident occurred. Target_amt is a numerical variable ranging from $0 (no accident/claims) upward.   

A binary logistic classification model will be constructed to estimate the probability of class membership (1 or 0) and a multiple linear regression model to predict payout cost in the event of an accident. This statistical approach builds on classical methods for model training/testing (which underlie more modern, machine learning, strategies for classification and prediction) and invites more explicit consideration of model assumptions and diagnostics. 

The data include the following variables: 

| Variable             | Description                                  |
|----------------------|----------------------------------------------|
| Target FLAG          | Was car in crash? 1=Yes, 0=No                |
| Target_ AMT          | If crash, what was the cost                  |
| AGE                  | Age of driver                                |
| BLUEBOOK             | Value of vehicle                             |
| CAR_AGE              | Age  of car                                  |
| CAR_TYPE             | Type of car                                  |
| CAR_USE              | Vehicle use                                  |
| CLM_FREQ             | # Claims (past 5 yrs)                        |
| EDUCATION            | Max educational level                        |
| HOMEKIDS             | # Children at home                           |
| HOME_VAL             | Home value                                   |
| INCOME               | Annual income                                |
| JOB                  | Job category                                 |
| KIDSDRIVE            | # Driving children                           |
| MSTATUS              | Martial status                               |
| MVR_PTS              | Motor vehicle record points                  |
| OLDCLAIM             | Total claims (past 5 yrs)                    |
| PARENT1              | Single parent                                |
| RED_CAR              | A red car                                    |
| REVOKED              | License revoked (past 7 yrs)                 |
| SEX                  | Gender                                       |
| TIF                  | Time as customer                             |
| TRAVTIME             | Distance to work                             |
| URBANICITY           | Home/work area                               |
| YOJ                  | Years on job                                 |

The project workflow will be represented and discussed following the approach of
[Wickham and Grolemund (2017)](https://r4ds.had.co.nz/):

! [Alt text] (/Users/seanc/Documents/Data_Projects/insurance_model/images/tidymodel.jpg)

- Import: libraries, raw data
- Tidy: cleaning, wrangling, tidy format
- Transform: type, relevel, summary statistics 
- Visualize: exploratory, diagnostic
- Model: model building, diagnostics, selection, evaluation
- Communicate: project summary, knitted document, portfolio

```{r}

# Import libraries

library(tidyverse) # cleaning & wrangling functions
library(janitor) # data cleaning
library(magrittr) # piping
library(flextable) #table layout and formatting
library(dlookr) # exploratory data analysis functions
library(patchwork) # easy plot layout
library(ggpubr) # creates a wrapper for plotting a list
library(viridis) #box plots
library(broom) # creates a tidy data frame from statistical test results
library(InformationValue) # optimize threshold
library(caret) # conf matrix
library(ROCR) #roc curve
library(car) # marginal plots
```

```{r}

# Read in data

path <- "https://raw.githubusercontent.com/sconnin/insurance_model/main/insurance_dataset.csv"

raw <- read_csv(path)

```
##1. Data Processing##

**1.1 Clean and Reformat Data**

Data processing is required to put variables in tidy form for model building as 
well as to support initial exploration and visualization. Additional steps are 
taken to facilitate interpretation of feature variables.    

The following processing steps are included below:

- data storage as dataframe(s)
- case consistency and removal of special characters
- type conversion and re-leveling of select factor variables
- removal of empty/duplicate rows and/or columns
- subsetting to remove unnecessary columns
- renaming column values for clarity

```{r}

# Copy data into working dataframe for downstream use

df<-raw%>%
    
    clean_names%>% # initial clean of col names
    
    remove_empty(c("rows", "cols"))%>%  # remove any empty rows and cols
    
    distinct()%>%     # remove duplicate rows
    
    mutate_if(is_character, str_replace_all, '\\$|,|z_|<', '')%>%  # clean any special chars in character variables

    dplyr::select(-index)%>%  # remove index

    mutate_if(is_character, str_replace_all, "No|no",'N')%>%  
    
    mutate_if(is_character, str_replace_all, "Yes|yes",'Y')%>%
    
    mutate_if(is_character, str_replace_all, "Highly Urban/ Urban",'Urban')%>%
    
    mutate_if(is_character, str_replace_all, "Highly Rural/ Rural",'Rural')%>%
    
    mutate_at(vars(income, home_val, bluebook, oldclaim), funs(as.numeric))%>%   # correct variable type: char to numeric

    mutate_if(is.numeric, round)%>%  # round out our numerics
    
    mutate_if(is_character, ~(factor(.)))%>%  # convert all character variables to factor for modeling
    
    mutate(education = fct_relevel(education, c("High School", "Bachelors", "Masters", "PhD")))%>% # relevel to show educational attainment steps
    
    mutate(car_type = fct_relevel(car_type, c("Sports Car", "SUV", "Minivan, Van", "Pickup", "Panel Truck")))

str(df) # review structure of data set

```

**1.2 Tabulate Count and Frequency of Target Variable**

Recall that the variable target_flag indicates whether or not an individual was in an
accident (1 = Yes, 0 = No).

* target_flag (level = 0) includes 6008 observations and accounts for ~74% of the data.

* target_flag (level = 1) includes 2153 observations and accounts for ~26% of the data. 


```{r}

# Subset levels of target_flag into new dataframes for analyses

target_0 <- df%>%
    dplyr::select(-target_amt)%>%
    filter(target_flag == 0) # obs not associated with automobile accidents

target_1 <- df%>%
    dplyr::select(-target_amt)%>%
    filter(target_flag == 1)  # obs associated with automobile accidents

# Calculate the proportion of data in each subset of target_flag

df %>%
    group_by(target_flag) %>%
    summarise(n = n()) %>%
    mutate(freq = n / sum(n))%>%
    flextable()%>%
    set_caption("Count and Frequency of Target Levels")

```
**1.3 Summary Statistics for Numerical and Categorical Variables**

On the basis of the following statistical summaries we find:

- There is one negative value (-3) for 'car_age' that should be investigated. This may be 
a data entry error.
- A number of numerical variables have highly skewed distributions (compare mean & median)
- A high percentage of zeros occur in variables: 'kidsdriv', 'homekids', 'oldclaim', 'clm_freq', 'mvr_pt'
- Outliers are common across the numerical variables, particularly for observations where target_flag = 0.  


```{r}

# Calculate statistics summary for numeric variables at each level of target_flag    

target_0%>%
    dplyr::select(-target_flag)%>%
    diagnose_numeric()%>%
    dplyr::select(variables, min, mean, median, max, zero, minus, outlier)%>%
    flextable()%>%
    set_caption("Summary Statistics for Target_Flag = 0")

target_1%>%
    dplyr::select(-target_flag)%>%
    diagnose_numeric()%>%
    dplyr::select(variables, min, mean, median, max, zero, minus, outlier)%>%
    flextable()%>%
    set_caption("Summary Statistics for Target_Flag = 1")

#Calculate percentage of zero values in numerical covariates for each level of target_flag

select_col<- as_mapper(~all(is.na(.)|.< 1)) # purrr mapper for possible downstream use

df%>%
    group_by(target_flag)%>%  # note target must be numerical type for the following to run
    dplyr::select(where(is.numeric) & -target_amt)%>%
    summarise_each(~round(sum(.==0)/length(.)*100))%>%
    purrr::discard(select_col)%>%
    flextable()%>%
    set_caption("Percentage of Zeros in Numerical Covariates by Level of Target_Flag")

# Create statistical summaries for categorical variables at each level of target_flag 

target_0%>%
    diagnose_category()%>%
    flextable()%>%
    set_caption("Summary Statistics for Categorical Variables: Target Level = 0")

target_1%>%
    diagnose_category()%>%
    flextable()%>%
    set_caption("Summary Statistics for Categorical Variables: Target Level = 1")





```

We drop one observation where 'car_age' is < 0 given that it is impossible to have
a negative age. This appears to be a data entry error given there are no other
negative values in this variable category. 

```{r}

# Drop observation with data error for car age (obs. value = -3). 

df$car_age[df$car_age < 0] <- NA


```

**1.4 Covariate Distributions**

Histograms and QQ-Plots indicate that distributions for 'target_flag', 'kidsdriv', 'homekids', 'yoj', 'income', 'oldclaim', 'clm_freq', and 'mvr_pts' depart from a normal model. This is confirmed using a Shapiro-Wilk test of normality for numerical values.

Transformation of these variables may be desirable during model building. Log, square root, and polynomial transformations may be particularly helpful in this regard. 

From an interpretive stand-point, it's worth noting the high 0 value counts for the following covariates: 'tif', 'oldclaim', 'car_age', 'income', 'home_val'. Particularly the latter two. For example, a 0 value for 'income' may indicate that an individual is unemployed and/or a student/dependent. A 0 value for 'home_val' may indicate that an individual is a renter, a dependent, or homeless. 

These details can be important in establishing the costs:benefits of insuring a driver. And their absence (as in this dataset) can limit the discriminatory ability of a model. 


```{r}

# Evaluate and rank departures from normality for numerical vars - Wilkes Shapiro

df %>%
    as.data.frame()%>%
    dplyr::select(-target_amt, target_flag)%>%
    normality() %>%
    filter(p_value <= 0.01) %>%
    arrange(abs(p_value))

#Identify skewness in covariates across levels of target_flag

target_0%>%find_skewness(index=FALSE, thres=TRUE) 
target_1%>%find_skewness(index=FALSE, thres=TRUE)

# Identify potential transformations to address skew, this does not resolve high 0 counts

df%>%
    dplyr::select(age, yoj, income, home_val, travtime, bluebook, tif, oldclaim, car_age)%>%
    plot_normality()

```
**1.5 Data Outliers**

The variables kidsrive, oldclaim, homekids, clm_freq, an yoj contain the highest percentage of outliers 
(~6-17%, depending on target_flag level). The percentage of outliers is generally greater for observations
associated with accidents (target_flag = 1) than without (target_flag = 0). 

The extent to which these/other outliers influence model parametrization is not yet know and should be
evaluated as part of the model building process.

```{r}

# Identify percentage of outliers for each covariate across levels of target_flag
    

diagnose_outlier(target_0) %>%
    filter(outliers_ratio > 5)%>%
    mutate(ratio_means= outliers_mean/with_mean)%>% # mean val of outliers/total mean
    arrange(desc(ratio_means))%>%
    dplyr::select(variables, outliers_ratio, ratio_means)%>%
    flextable()%>%
    set_caption("Outlier Summary: Target_Flag = 0")

diagnose_outlier(target_1) %>%
    filter(outliers_ratio > 5)%>%
    mutate(ratio_means= outliers_mean/with_mean)%>%
    arrange(desc(ratio_means))%>%
    dplyr::select(variables, outliers_ratio, ratio_means)%>%
    flextable()%>%
    set_caption("Outlier Summary: Target_Flag = 1")
```

**1.6 Missing Data**

Our dataset includes missing observations in one categorical variable ('job') and six
numerical variables ('car_age', 'home-val', 'yoj', 'income', 'age'). Across covariates 
(i.e., predictor variables) the extent of missingness is < 6.5% of the total 
observations. There are no obvious patterns in the missing data that warrant concern.

```{r }

#Basic missing tables by target level
    
target_0%>%
    diagnose()%>%
    dplyr::select(-unique_count, -unique_rate)%>%
    filter(missing_count>0)%>%
    arrange(desc(missing_count))%>%
    flextable()%>%
    set_caption("Missing Data Summary: Target Level = 0")
    

target_1%>%
    diagnose()%>%
    dplyr::select(-unique_count, -unique_rate)%>%
    filter(missing_count>0)%>%
    arrange(desc(missing_count))%>%
    flextable()%>%
    set_caption("Missing Data Summary: Target Level = 1")
    

# Plot missing data to visualize cumulative percentages and col intersections

df %>% 
  plot_na_pareto(only_na = TRUE)

df %>%
    plot_na_intersect()  



```
**1.7 Impute Missing Values**

Given the relatively limited extent of missing data, an argument can be made for
imputing these values rather than removing the observations during model building. 
A variety of imputation methods are availabe for this purpose. They include:

- random selection of a value(s) between the minimum and maximum value of a covariate
- replacement with the mean, median, or mode of the covariate's distribution
- linear regression based on other covariates
- k-nearest neighbors
- recursive partitioning and regression trees
- multivariate imputation by chained equations (mice)

The following methods are selected for our model on the basis of initial experimentation:

- 'car _age': mice
- 'home_val', 'yoj', 'income', 'age': recursive partitioning

Comparison of density plots for original and imputed values show a good fit 
for each of the numerical covariates.

```{r}

#Impute numerical vars using dlookr

#car_age

car_age<-imputate_na(df, car_age, target_flag, method = "mice", seed = 999)

p1<-plot(car_age)+theme_minimal()+theme(legend.position = "top")

summary(car_age)


#home_val


home_val<-imputate_na(df, home_val, target_flag, method = "rpart")

p2<-plot(home_val)+theme_minimal()+theme(legend.position = "top")

summary(home_val)

#yoj


yoj<- imputate_na(df, yoj, target_flag, method = "rpart")

p3<-plot(yoj)+theme_minimal()+theme(legend.position = "top")

summary(yoj)

# income

income<-imputate_na(df, income, target_flag, method = "rpart")

p4<-plot(income)+theme_minimal()+theme(legend.position = "top")

summary(income)

#age

age<-imputate_na(df, age, method = "rpart")

p5<-plot(age)+theme_minimal()+theme(legend.position = "top")

summary(age)

temp<-cbind(car_age, home_val, yoj, income, age)
temp%<>%as.data.frame(temp)

df%<>%dplyr::select(!c(car_age, home_val, yoj, income, age))%>%
    cbind(temp)

df%<>%mutate_if(is.numeric, round)

# print plots

(p1|p2)/(p3|p4)/p5


```
We can also impute missing values for categorical variables. In this case, the 
mice method is used to impute values for the variable, 'job'. 

```{r}

job<-imputate_na(df, job, method = "mice", seed = 999)

plot(job)+theme_minimal()+theme(legend.position = "top")


# combine into new dfb

df<-df%>%
    dplyr::select(!job)

df<-cbind(df,job)

df$job<-factor(df$job)

```
Save csv with imputed values 

```{r}

#df%<>%mutate_at(vars(target_flag), funs(factor)) # convert target_flag to factor

#write.csv(df, 'df_clean.csv', row.names=FALSE)

```

**1.8 Pairwise Comparisons**

Pairwise comparisons provide a method to identify collinear relationships between
covariates. Covariates that display high collinearity (e.g., correlation coefficients
< -.50  or > .50) can reduce the precision and increase the sensitivity of model 
coefficients due to lack of independence.     

To refine our analyses, pairwise comparisons are assessed following removal of 
0 values. Our results indicate:

- high positive correlation between 'home_val' and 'income', coeff ~ 0.96
- high positive correlation between 'homekids' and 'homedrive', coeff ~ 0.62
- moderate positive correlation between 'home_val' and 'car_age', coeff ~ 0.54
- moderate positive correlation between 'income' and 'car_age', coeff ~ 0.54

As expected, there is also a strong association between 'income' (numerical var.) 
and 'job' type (factor var.) - based on a kruskal-Wallis rank sum test.

Both 'home_value' and 'income' have similar levels of missing data. An argument can be
made for dropping one or the other from the dataset in order to simplify 
our feature selection process and improve model precision. This may also be
true for 'income' and 'job' type.

Associations between 'home_val' and 'car_age' and 'income' and 'car_age' are less 
concerning, due to lower correlation coefficients and higher variability. See
plots below.

Less clear is the association between 'homekids' and 'kidsdriv'
given their narrow range of integer values. 

We will revisit options for feature removal during the model building process.


```{r}

#Assess collinearity in the absence of zero values in numerical cols

df%>%
    filter_if(is.numeric, all_vars((.) != 0))%>%
    correlate()%>%
    filter(coef_corr > .5 | coef_corr < -.5)%>% # set thresholds to limit output 
    flextable()%>%
    set_caption("Pairwise Correlation: Numerical Covariates")

# Plot relationship between numerical covariates (w/o zero values for clarity) beyond the threshold coef_corr 

df %>%
    filter(income != 0 & home_val != 0)%>% 
    target_by(income) %>%
    relate(home_val) %>%
    plot()
    

df %>%
    filter(homekids != 0 & kidsdriv != 0)%>%
    target_by(homekids) %>%
    relate(kidsdriv) %>%
    plot()

df %>%
    filter(home_val != 0 & car_age != 0)%>%
    target_by(home_val) %>%
    relate(car_age) %>%
    plot()

df %>%
    filter(income != 0 & car_age != 0)%>%
    target_by(income) %>%
    relate(car_age) %>%
    plot()


# Plot relationship between numerical and factors for select variables

df %>% 
  target_by(income) %>%      
  relate(job) %>% 
  plot()

df%>%kruskal.test(income~job) # assess covariance with Kruskal-Wallis rank sum test where > 1 categorical level


```
We can visually assess relationships between categorical variables and 
our response variable, 'target-flag', using mosaic plots. In this case,
a relationship is indicated by relative differences in variable 
levels for each level of 'target-flag'.

Note that the width of plot columns is proportional to the count of observations
for each level of the variable plotted on the horizontal axis. And the vertical
lenght is proportional to the count in the second variable (i.e., 'target_flag')
within each level of the first variable. 

On this basis we find that:

- All categorical variables except 'sex' and 'red-car' display relationships 
to 'target_flag'.

An argument can be made for dropping 'sex and 'red_car' from our set of covariates
in order to streamline the modeling process. 

```{r }

# Subset categorical variables into a df of factor vars

mosaic<-df%>% #subset target_flag as factor
    mutate_at(vars(target_flag), funs(as.factor))%>%
    dplyr::select(where(is.factor))

target <- all_of(names(mosaic)[1]) # set name for target_flag
predictors <- all_of(names(mosaic)[2:11])

#generate mosaic plots with purrr and dlookr

(plots<-predictors%>%map(function(predictors) mosaic%>%target_by(target)%>%relate(predictors)%>%plot()))




```
Boxplots make an effective visual tool to assess relationships between 'target_flag'
and our numerical covariates. 

On this basis, it appears that 'bluebook', 'tif', 'clm_freq', 'mvr_pts', 'car_age',
'home_val', and 'income' vary by level of target_flag. The presence of outliers,
however, argues for caution in this interpretation. 


```{r}

# Subset numerical variables

num_box<-select_if(df, is.numeric)%>%
    mutate_at(vars(target_flag), funs(as.factor))%>%
    dplyr::select(-target_amt)

# Plot using boxplots

response = names(num_box)[1] #target_flag (needs to be fct for these plots)
response = purrr::set_names(response)

explain <- names(num_box)[2:14] #explanatory variables
explain = purrr::set_names(explain)

box_fun = function(x) {
    ggplot(num_box, aes_string(x = x, y = 'target_flag') ) +
    geom_boxplot(aes(fill = target_flag, alpha = 0.4), outlier.color =
    'red', show.legend = FALSE)+
    scale_fill_viridis(discrete = TRUE, option = "E")+
    coord_flip()+
    theme_classic()
}

box_plots<-map(explain, ~box_fun(.x)) #creates a list of plots using purrr

plot(reduce(box_plots,`+`)) # layout with patchwork
```
**1.9 Final Variable Selections**

Before forming training and testing datasets, we will drop 'home_val' and 
'target_amt'. Variability in the model explained by 'home_val' will be largely
explaine by 'income'. And 'target_amt'  is not relevant to our classification 
process.

We will also collapse the variable 'job' into two levels: 'non-white-collar' and 
'white-collar'. While this may result in a slight loss of discriminatory power in 
our model(s) it does lend to model simplification.

Rather than make any additional a-priori choices to drop other covariates, we will 
automate variable selection (via. Akaike Information Criterion) as part of the 
modeling process.  


```{r}

base_df<-df%>%
    dplyr::select(-home_val, -target_amt)  # variance in these fields explained by income

base_df%<>%
    mutate(job = (fct_collapse(job,
                 not_white_collar = c("Student", "Blue Collar", "Home Maker", "Clerical", "Manager"),
                 white_collar = c("Doctor", "Lawyer", "Professional"))))

#write.csv(base_df, 'base_df.csv', row.names=FALSE)    
```

##2. Training and Test Datasets##

Prior to modeling, the dataset is randomly sampled to create a subset for model training (train_df) and model evaluation (test_df). 

```{r}
# Base_df<-read.csv('base_df.csv') # hold here to assist model dev 

base_df%>%count(target_flag)%>%
    mutate(frequency = n/sum(n))%>%
    flextable()%>%  
    set_caption("Count and Frequency of Target Variable by Level")

set.seed(1235) 

# Build train/test sets

trainIdx <- createDataPartition(base_df$target_flag, p = .75,
            list = FALSE, times = 1)

train_df <- base_df[trainIdx,]
test_df <- base_df[-trainIdx,]

#Check counts and frequencies of target_flag for train set and test set 

train_df%>%count(target_flag)%>%
    mutate(frequency = n/sum(n))%>%
    flextable()%>%
    set_caption("Count and Frequency of Target Variable: Training Data")

test_df%>%count(target_flag)%>%
    mutate(frequency = n/sum(n))%>%
    flextable()%>%
    set_caption("Count and Frequency of Target Variable: Test Data")

```

##3. Logistic Regression Models##

Selection of a logistic classifier will be based on comparison of four 
models:

1. Null model - the class probability p is independent of any terms (covariates) and is 
the same for all data points. Only one parameter (the intercept) is fit. 
2. Full model - all possible terms are included in the model.
3. Reduced model - a nested subset of a the full model that excludes non-significant (chi-sqr.).
4. Transformation model - select terms used in the the reduced model are transformed to
meet model requirements and improve fit.

We will assume that the null and full models are valid for the purposes of model comparison. Validity will be inferred from model diagnostics for the reduced and transformed model. 

To be considered valid, a binary logistic model should meet the following
conditions:

- the response variable is binary.
- observations are independent of each other.
- there is little or no multicollinearity among covariates.
- covariates and the logit (log odds) are linearly related.


**3.1 Null Model: Logistic Regression**

```{r}

null<-glm(target_flag ~ 1, family = binomial, train_df)

summary(null)

null_stats<-glance(null) # collect stats for model comparisons

```

**3.2 Full Model: Logistic Regression**

Several terms included in the full model are not significant - 'homekids', 'sex', 
'red_car', 'car_age', 'age'. 

```{r}

full<-glm(target_flag ~ ., family = binomial, train_df) 

summary(full)


full_stats<-glance(full) # collect stat metrics for model comparisons

```
**3.3 Reduced Model: Logistic Regression **

The reduced model will be constructed using the Akaike Information Criterion to 
remove covariates that are not statistically significant. 

```{r}

#Build reduced model

reduced <- glm(target_flag ~ .,family='binomial', train_df) 

reduced<-step(reduced, trace=0) # use Akiaike step for covariate selection, trace 0 prevents intermediate printing

summary(reduced)

reduced_stats<-glance(reduced) # collect stats for model comparisons

```
**3.3.1 Diagnostics: Reduced Model**

The ability of a binary classifier to discriminate class membership can be 
evaluated using a confusion (error) matrix as well as a receiver operating 
characteristic (ROC curve).

A confusion matrix for binary classifiers consists of a 2x2 table with counts of 
actual and predicted classes for a response variable; providing a measure of
correct (true positives, true negatives) and incorrect classifications 
(false positives, false negatives) at a given probability threshold.

The receiver operating characteristic plots a model's true positive rate 
(i.e., sensitivity|recall|probability of detection) against the false positive 
rate (i.e., 1-specificity|probability of false detection) at various probability thresholds. Importantly, the area under the ROC curve (AUC) offers a measure of 
model accuracy and can be used as a statistic for model comparison. 

The reduced model correctly classifies ~ 75% of the positive class and ~67% of
the negative class for 'target_flag' at a probability threshold of 0.51. In this
instance, the selected threshold optimizes for balance in true positive and 
false positive rates. 

Depending on claim costs, it may be advisable to lower the threshold such that
more positive observations (crashes) are correctly classified at the cost of
fewer correct negative classifications. Translated, this reflects a cost:balance
between potential payouts and income that can guide a carrier's determination of 
coverage for an individual driver. 

The AUC and other measures of performance for the reduced model are discussed in
the model selection section. 

```{r}

# Collect predicted percentages

predicted<-predict(reduced,train_df,type='response')

# Find optimal threshold to minimize misclassification

optimal<- optimalCutoff(train_df$target_flag, predicted, optimiseFor = "misclasserror")  # 0.51

# Calculate training classification error with .51 threshold --> 0.2595

Miss_Class_Error<-misClassError(train_df$target_flag, predicted,  threshold = optimal)

# Convert predicted to 0, 1 values based on threshold

predicts<-ifelse(predicted >= optimal, 1, 0)

# Subset actuals and predicts as numeric for confusion matrix (factors in caret)

actuals<-as.numeric(as.character(train_df$target_flag))

(InformationValue::confusionMatrix(actuals, predicts, threshold = optimal)) 

#Calculate related metrics

AUC<-AUROC(actuals, predicted) # -->.81

Sensitivity<-InformationValue::sensitivity(actuals, predicts, threshold = optimal) # recall: true positive rate
Specificity<-InformationValue::specificity(actuals, predicts, threshold = optimal) #false positive rate
Precision<-InformationValue::precision(actuals, predicts, threshold = optimal) # prop of predicted ones /prop total ones

YDI_Index<- youdensIndex(actuals, predicts, threshold = optimal) # 0.48 accounts for both false-positive and false-negative rates

# Assess predictive power using concordance

Predict_Power<-Concordance(actuals, predicts) # predictive power  ---> .548
Predict_Power<-Predict_Power$Concordance

ks_plot(actuals, predicts) #cum perc of ones captured by the model against that expected random

Kol_Smirnov<-ks_stat(actuals, predicts) #Kolmogorov-Smirnov statistic - maximum difference between the cumulative true positive and cumulative false positive rate. --> .69

# Create a metrics table

reduced_metrics<-cbind(Predict_Power, Miss_Class_Error, AUC, Sensitivity, Specificity, Precision, YDI_Index, Kol_Smirnov)

# Construct ROC curve: note actuals, predicts, predicted as vectors)

pred <- ROCR::prediction(predicted, actuals)
perf <- ROCR::performance(pred, "tpr", "fpr")

plot(perf, colorize = TRUE, main = "ROC curve for logistic regression on insurance data")

#AUC curve

plotROC(actuals, predicted)


```

**3.3.2 Reduced Model: Overdispersion**

As part of the diagnostic process, we check the model residuals for evidence of
overdispersion. This occurs when the residuals are more variable than expected
for a binomial distribution. 

Overdispersion can be evaluated by dividing a model's residual deviance by the 
degrees of freedom. A value close to 1 indicates that there is no overdispersion.
A greater value indicates otherwise; in which case a negative binomial model may
be more appropriate. 

There is no evidence of overdispersion in the reduced model.

```{r}

# Evaluate using deviance and quasibinomial comparison

deviance(reduced)/df.residual(reduced) # if considerably greater than one we should be concerned

# Check with two model fit

quasi_model <-  glm(target_flag ~ .,family='quasibinomial', base_df) # note: using base_df
  
pchisq(summary(quasi_model)$dispersion * reduced$df.residual,
 reduced$df.residual, lower = F)  
```

**3.3.3 Reduced Model: Outliers and Infuential obs**

The presence of influential outliers (outliers with high leverage) can negatively
affect model results. 

Influence can be evaluated on the basis of a model's 
[standardized residuals](https://www.statology.org/standardized-residuals/) and 
[Cook's distance](http://www.sthda.com/english/articles/36-classification-methods-essentials/148-logistic-regression-assumptions-and-diagnostics-in-r/).

While the majority of covariates contain outliers, there is little evidence for 
influential outliers (excepting obs. 5101) in the training set 
(i.e., st.resid <3 stdev & cooksd < 1). 


```{r}

# Extract model results

reduced_df<-train_df

reduced_df$data <- augment(reduced) %>% #add model output to dataframe
  mutate(index = 1:n()) 

# Identify influential outliers

reduced_df$data %>% top_n(10, .cooksd)

plot(reduced, which = 4, id.n = 3)  # keep an eye on obs > 4/n 

#Plot Standardized Residuals

ggplot(reduced_df$data, aes(index, .std.resid)) + 
  geom_point(aes(color = target_flag), alpha = .5) +
  theme_bw()

#Filter potential influential data points with abs(.std.res) > 3

reduced_df$data %>% 
  filter(abs(.std.resid) > 3)


```
**3.3.4 Reduced Model: linearity of numerical variables vs. logit**

Recall that linearity between each numerical covariate and the logit is one 
requirement of a valid logistic model. 

The bivariate plots below indicate this requirement is met for most of the 
covariates, with the possible exception of 'travtime' and 'oldclaim'. These 
variables may benefit from transformation. 

```{r}

#create a dataframe for linearity analysis

reduced_df$predicted<-predict(reduced,train_df,type='response') # add predicted

reduced_df%<>%mutate(logit = log(predicted/(1-predicted))) # add logit


# plot - check linearity btwn numerical predictors and logit, .data used to pass strings into aes

covar_df<-reduced_df%>%
    dplyr::select(is.numeric)

xvar<-names(covar_df)[2:13] # numerical covariates
xvar<-purrr::set_names(xvar)

linear_func <- function(x){
    ggplot(covar_df, aes_string(x=x, y='logit'))+ 
    geom_point(alpha=.2)+
    geom_smooth(method = "loess", se = FALSE, color='red')+
    theme_minimal()
}

linear_plots<-map(xvar, ~linear_func(.x)) #map plotting function

plot(reduce(linear_plots,`+`)) # reduce for single print


```

**3.3.5. Reduced Model: Collinearity**

Another requirement of a valid logistic model is the absence of multicollinearity
among covariates. One measure of multicollinearity is the Variance Inflation 
Factor (VIF). A high VIF value (e.g. 10) for a covariate indicates that it is
highly collinear with other covariates in the model. 

There is no evidence of problematic multicollinearity in the reduced model. 

```{r}

car::vif(reduced)
```

**3.3.6. Reduced Model: Independence of Residuals**

A final model requirement is the independence of observations. This can be
assess by plotting the model residuals against the linear predictor (i.e., combination
of coefficients and covariates). Independence is indicated by the absence of 
pattern in the resulting plot. 

There does appear to be some curvature in the plot of residuals vs linear
predictor for the reduced model that may point to a departure from independence. 
This might result, for example, from grouping effects among categorical 
covariates. 

Given that the plotting pattern is not severe and that the model serves a 
predictive (vs. explanatory) purpose, there is not sufficient evidence to reject
the model.

```{r }

resid_df<-train_df%>%mutate('residuals' = residuals(reduced), linpred = predict(reduced))

bins<- group_by(resid_df, cut(linpred, breaks=unique(quantile(linpred, (1:100)/101))))

diag<-summarize(bins, residuals=mean(residuals), linpred = mean(linpred))

plot(residuals~linpred, diag, xlab = "linear predictor")


```

**3.3.7 Reduced Model: Goodness of Fit - Marginal Plots**

The reduced model is considered valid on the basis of requirements described 
for logistic regression. 

The 'goodness' of model fit can be readily evaluated from marginal plots for each variable. Marginal plots graph actual and conditional probabilities (estimated by the model) 
for each covariate relative to the response variable. The resulting curves align 
closely in a model that fits the data well.

The reduced model provides a good fit to the data. Although some departure
is observed for actual and conditional probabilities in the plots for 'travtime',
'bluebook', and 'age'. These departures are not severe and may be lessened by
transforming these features. 
 
```{r}
marginals_reduced<-mmps(reduced,main=NULL)
```

**3.4 Transformation Model**

The transformation model includes terms from the reduced model with
transformations to reduce skew in select covariates. On this basis, 
'travtime' and 'bluebook' will be log transformed while 'tif' and 'income'
will be square root transformed. 

This transformations follow from earlier model trials (not shown here) and 
experimentation. 

All covariates included in the transformation model are significant at the 
.05 level. 

The transformation model correctly classifies ~ 80% of the positive class and ~68% of
the negative class for 'target_flag' at a probability threshold of 0.21 - optimizing
for low misclassification error. The AUC is ~ .81, similar to the reduced model.

The transformation model is valid (diagnostics not included for the purpose of 
brevity).

```{r fig.align='center', fig.height=10,fig.width=10, message = FALSE}


#Build model with transformed vars

trans <- glm( target_flag ~ kidsdriv + parent1 + mstatus + education + 
    log(travtime) + car_use + log(bluebook) + sqrt(tif) + car_type + oldclaim + 
    clm_freq + revoked + mvr_pts + urbanicity + sqrt(income), family='binomial', train_df)

trans<-step(trans, trace=0) 

summary(trans)

trans_stats<-glance(trans) # save stats for model comparisons


```
**3.4.1 Transformation Model: Diagnostics **



```{r}

# collect predicted percentages

trans_predicted<-predict(trans, train_df,type='response')

# find optimal threshold to minimize misclassification

trans_optimal<- optimalCutoff(train_df$target_flag, trans_predicted, optimiseFor = "misclasserror")  # 0.51

# calculate training classification error with .51 threshold --> 0.2588

Miss_Class_Error<-misClassError(train_df$target_flag, trans_predicted,  threshold = trans_optimal)

# convert predicted to 0, 1 values based on threshold

trans_predicts<-ifelse(trans_predicted >= trans_optimal, 1, 0)

# subset actuals and predicts as numeric for confusion matrix

trans_actuals<-as.numeric(as.character(train_df$target_flag))

InformationValue::confusionMatrix(trans_actuals, trans_predicts, threshold = optimal)%>%
    flextable()%>%
    set_caption("Confusion Matrix: Reduced Model")

#Calculate related metrics

AUC<-AUROC(trans_actuals, trans_predicted) # -->.81
Sensitivity<-InformationValue::sensitivity(trans_actuals, trans_predicts, threshold = optimal) # recall: true positive rate
Specificity<-InformationValue::specificity(trans_actuals, trans_predicts, threshold = optimal) #false positive rate
Precision<-InformationValue::precision(trans_actuals, trans_predicts, threshold = optimal) # prop of predicted ones /prop total ones
YDI_Index<-youdensIndex(trans_actuals, trans_predicts, threshold = optimal) # 0.48 accounts for both false-positive and false-negative rates

# Calculate predictive power using concordance

Concord<-Concordance(trans_actuals, trans_predicts)
Predict_Power<-Concord$Concordance

ks_plot(trans_actuals, trans_predicts) #cum perc of ones captured by the model against that expected random

Kol_Smirnov<-ks_stat(trans_actuals, trans_predicts) #Kolmogorov-Smirnov statistic - maximum difference between the cumulative true positive and cumulative false positive rate

# Create a metrics table

trans_metrics<-cbind(Predict_Power, Miss_Class_Error, AUC, Sensitivity, Specificity, Precision, YDI_Index, Kol_Smirnov)

# Construct ROC curve: note actuals, predicts, predicted as vectors)

pred_trans <- ROCR::prediction(trans_predicted, trans_actuals)
perf_trans <- ROCR::performance(pred_trans, "tpr", "fpr")

plot(perf_trans, colorize = TRUE, main = "ROC curve for logistic regression on insurance data")

#AUC curve

plotROC(trans_actuals, trans_predicted)
```
**3.4.2 Transformed Model: Goodness of Fit **

Marginal plots for show improvements in model fit with the transformations 
selected for 'travtime', 'bluebook', 'tif', and 'income'.

```{r}
marginals_trans<-mmps(trans,main=NULL)
```


##4. Model Selection##

Note - trans - marginals similar to reduced, collinearity similar, linearity similar

```{r}

# Compare model output summaries

col_name<-c('Null Model', 'Full Model', 'Reduced Model', 'Transformed Model')

comp<-rbind(null_stats, full_stats, reduced_stats, trans_stats)

(compare_models<- cbind(col_name, comp)%>%
    dplyr::select(-c(null.deviance, df.null))%>%
    rename(Model = col_name)%>%
    flextable()%>%
     set_caption("Comparison of Model Results"))

# Compare select metrics for reduced and transformed models

col_final<-c('Reduced Model', 'Transformed Model')

final_comp<-rbind(reduced_metrics, trans_metrics)

(final_models<- cbind(col_final, final_comp)%>%
    as.data.frame()%>%
    mutate_at(vars(2:9), funs(as.numeric))%>%
    mutate_if(is.numeric, round, 3)%>%
    rename(Model = col_final)%>%
    flextable()%>%
    set_caption("Classification Metrics: Reduced & Transformed Models"))
```

##5. Model Evaluation##

```{r}

# Apply reduced model to test data

reduced_test<-glm(formula = target_flag ~ kidsdriv + parent1 + mstatus + 
    education+travtime + car_use + bluebook + tif + car_type + oldclaim + 
    clm_freq + revoked + mvr_pts + urbanicity + income + age, 
    family = "binomial", data = test_df)

# Apply fitted model to test sample (predicted probabilities)

predicted_test <- predict(reduced_test, test_df, type="response")

# Find optimal threshold to minimize misclassification

test_optimal<- optimalCutoff(test_df$target_flag, predicted_test, optimiseFor = "Both") # --> .149

# Calculate training classification error with .51 threshold

Miss_Class_Error<-misClassError(test_df$target_flag, predicted_test,  threshold = test_optimal)

# Convert predicted to 0, 1 values based on threshold

predicts_test<-ifelse(predicted_test >= test_optimal, 1, 0)

# Cubset actuals and predicts as numeric for confusion matrix

actuals_test<-as.numeric(as.character(test_df$target_flag))

InformationValue::confusionMatrix(actuals_test, predicts_test, threshold = test_optimal)%>%
    flextable()%>%
    set_caption("Confusion Matrix: Transformation Model")

#Calculate related metrics

AUC<-AUROC(actuals_test, predicted_test) # -->.81
Sensitivity<-InformationValue::sensitivity(actuals_test, predicts_test, threshold = .17) # recall: true positive rate
Specificity<-InformationValue::specificity(actuals_test, predicts_test, threshold = .17) #false positive rate
Precision<-InformationValue::precision(actuals_test, predicts_test, threshold = .17) # prop of predicted ones /prop total ones
YDI_Index<-youdensIndex(actuals_test, predicts_test, threshold = test_optimal) # 0.48 accounts for both false-positive and false-negative rates

# Calculate predictive power using concordance

Concord<-Concordance(actuals_test, predicts_test)
Predict_Power<-Concord$Concordance

ks_plot(actuals_test, predicts_test) #cum perc of ones captured by the model against that expected random

Kol_Smirnov<-ks_stat(actuals_test, predicts_test) #Kolmogorov-Smirnov statistic - maximum difference between the cumulative true positive and cumulative false positive rate

# Create a metrics table

(test_metrics<-cbind(Predict_Power, Miss_Class_Error, AUC, Sensitivity, Specificity, Precision, YDI_Index, Kol_Smirnov)%>%
    as.data.frame()%>%
    mutate_at(vars(1:8), funs(as.numeric))%>%
    mutate_if(is.numeric, round, 3)%>%
    flextable()%>%
    set_caption("Reduced Model-Test Data: Metrics"))
    

# Construct ROC curve: note actuals, predicts, predicted as vectors)

pred <- ROCR::prediction(predicted_test, actuals_test)
perf <- ROCR::performance(pred, "tpr", "fpr")

plot(perf, colorize = TRUE, main = "ROC curve for logistic regression on insurance data")

#Construct AUC curve

plotROC(actuals_test, predicted_test)

```

##6. Summary##

To be completed