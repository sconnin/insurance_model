---
title: "Insurance Classification and Prediction"
date: "1/11/22"
author: "Sean Connin"
output:
  html_document: 
    toc: true
    toc-title: ""
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: false
    theme: united
    highlight: tango
  pdf_document: default
---

```{r setup, include=FALSE}

# set global options

knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message = FALSE)

```
##Project Description##

The purpose of this work is to estimate the probability that an individual (seeking auto insurance) will be in an accident and then to forecast the potential cost of an ensuing claim. A synthetic data set of ~ 8000 observations will be used to construct predictive models for this purpose. The use of synthetic data permits model development absent proprietary information, while also providing a heuristic for quantitative reasoning.

The dataset includes two response variables. The first, 'target_flag', is scored as either a 1 or 0 indicating that an individual has or has not been in an accident, respectively. On this basis, a score of 1 is considered a 'positive' result and 0 a 'negative' result. The second response variable, 'target_amt' refers to the amount paid on a individual's claim given an accident occurred. Target_amt is a numerical variable ranging from $0 (no accident/claims) upward.   

A binary logistic classification model will be constructed to estimate the probability of class membership (1 or 0) and a multiple linear regression model to predict payout cost in the event of an accident. This statistical approach builds on classical methods for model training/testing (which underlie more modern, machine learning, strategies for classification and prediction) and permits more explicit consideration of model assumptions and diagnostics. 

The data include the following variables: 

| Variable             | Description                                  |
|----------------------|----------------------------------------------|
| Target FLAG          | Was car in crash? 1=Yes, 0=No                |
| Target_ AMT          | If crash, what was the cost                  |
| AGE                  | Age of driver                                |
| BLUEBOOK             | Value of vehicle                             |
| CAR_AGE              | Age  of car                                  |
| CAR_TYPE             | Type of car                                  |
| CAR_USE              | Vehicle use                                  |
| CLM_FREQ             | # Claims (past 5 yrs)                        |
| EDUCATION            | Max educational level                        |
| HOMEKIDS             | # Children at home                           |
| HOME_VAL             | Home value                                   |
| INCOME               | Annual income                                |
| JOB                  | Job category                                 |
| KIDSDRIVE            | # Driving children                           |
| MSTATUS              | Martial status                               |
| MVR_PTS              | Motor vehicle record points                  |
| OLDCLAIM             | Total claims (past 5 yrs)                    |
| PARENT1              | Single parent                                |
| RED_CAR              | A red car                                    |
| REVOKED              | License revoked (past 7 yrs)                 |
| SEX                  | Gender                                       |
| TIF                  | Time as customer                             |
| TRAVTIME             | Distance to work                             |
| URBANICITY           | Home/work area                               |
| YOJ                  | Years on job                                 |

The project workflow will be represented and discussed following the approach of
[Wickham and Grolemund (2017)](https://r4ds.had.co.nz/):

! [Alt text] (/Users/seanc/Documents/Data_Projects/insurance_model/images/tidymodel.jpg)

- Import: libraries, raw data
- Tidy: cleaning, wrangling, tidy format
- Transform: type, relevel, summary statistics 
- Visualize: exploratory, diagnostic
- Model: model building, diagnostics, selection, evaluation
- Communicate: project summary, knitted document, portfolio

```{r}

# Import libraries

library(tidyverse) # cleaning & wrangling functions
library(janitor) # data cleaning
library(magrittr) # piping
library(flextable) #table layout and formatting
library(dlookr) # exploratory data analysis functions
library(patchwork) # easy plot layout
library(ggpubr) # creates a wrapper for plotting a list
library(viridis) #box plots
library(broom) # creates a tidy data frame from statistical test results
library(InformationValue) # optimize threshold
library(caret) # conf matrix
library(ROCR) #roc curve
library(car) # marginal plots
```

```{r}

# Read in data

path <- "https://raw.githubusercontent.com/sconnin/insurance_model/main/insurance_dataset.csv"

raw <- read_csv(path)

```
##1. Data Processing##

**1.1 Clean and Reformat Data**

Data processing is required to put variables in tidy form for model building as 
well as to support initial exploration and visualization. Additional steps are 
taken to facilitate interpretation of feature variables.    

The following processing steps are included below:

- data storage as dataframe(s)
- case consistency and removal of special characters
- type conversion and re-leveling of select factor variables
- removal of empty/duplictae rows and/or columns
- subsetting to remove unnecessary columns
- renaming columns and values for clarity

```{r}

# Copy data into working dataframe for downstream use

df<-raw%>%
    
    clean_names%>% # initial clean of col names
    
    remove_empty(c("rows", "cols"))%>%  # remove any empty rows and cols
    
    distinct()%>%     # remove duplicate rows
    
    mutate_if(is_character, str_replace_all, '\\$|,|z_|<', '')%>%  # clean any special chars in character variables

    dplyr::select(-index)%>%  # remove index

    mutate_if(is_character, str_replace_all, "No|no",'N')%>%  
    
    mutate_if(is_character, str_replace_all, "Yes|yes",'Y')%>%
    
    mutate_if(is_character, str_replace_all, "Highly Urban/ Urban",'Urban')%>%
    
    mutate_if(is_character, str_replace_all, "Highly Rural/ Rural",'Rural')%>%
    
    mutate_at(vars(income, home_val, bluebook, oldclaim), funs(as.numeric))%>%   # correct variable type: char to numeric

    mutate_if(is.numeric, round)%>%  # round out our numerics
    
    mutate_if(is_character, ~(factor(.)))%>%  # convert all character variables to factor for modeling
    
    mutate(education = fct_relevel(education, c("High School", "Bachelors", "Masters", "PhD")))%>% # relevel to show educational attainment steps
    
    mutate(car_type = fct_relevel(car_type, c("Sports Car", "SUV", "Minivan, Van", "Pickup", "Panel Truck")))

str(df) # review structure of data set

```

**1.2 Tabulate Count and Frequency of Target Variable**

Recall that the variable target_flag indicates whether or not an individual was in an
accident (1 = Yes, 0 = No).

* target_flag (level = 0) includes 6008 observations and accounts for ~74% of the data.

* target_flag (level = 1) includes 2153 observations and accounts for ~26% of the data. 


```{r}

# Subset levels of target_flag into new dataframes for analyses

target_0 <- df%>%
    dplyr::select(-target_amt)%>%
    filter(target_flag == 0) # obs not associated with automobile accidents

target_1 <- df%>%
    dplyr::select(-target_amt)%>%
    filter(target_flag == 1)  # obs associated with automobile accidents

# Calculate the proportion of data in each subset of target_flag

df %>%
    group_by(target_flag) %>%
    summarise(n = n()) %>%
    mutate(freq = n / sum(n))%>%
    flextable()%>%
    set_caption("Count and Frequency of Target Levels")

```
**1.3 Summary Statistics for Numerical and Categorical Variables**

On the basis of the following statistical summaries we find:

- There is one negative value (-3) for car_age that should be investigated. This may be 
a data entry error.
- A number of numerical variables have highly skewed distributions (compare mean & median)
- A high percentage of zeros occur in variables: kidsdriv, homekids, oldclaim, clm_freq, mvr_pt
- Outliers are common across the numerical variables, particularly for observations where target_flag = 0.  


```{r}

# Calculate statistics summary for numeric variables at each level of target_flag    

target_0%>%
    dplyr::select(-target_flag)%>%
    diagnose_numeric()%>%
    dplyr::select(variables, min, mean, median, max, zero, minus, outlier)%>%
    flextable()%>%
    set_caption("Summary Statistics for Target_Flag = 0")

target_1%>%
    dplyr::select(-target_flag)%>%
    diagnose_numeric()%>%
    dplyr::select(variables, min, mean, median, max, zero, minus, outlier)%>%
    flextable()%>%
    set_caption("Summary Statistics for Target_Flag = 1")

#Calculate percentage of zero values in numerical covariates for each level of target_flag

select_col<- as_mapper(~all(is.na(.)|.< 1)) # purrr mapper for possible downstream use

df%>%
    group_by(target_flag)%>%  # note target must be numerical type for the following to run
    dplyr::select(where(is.numeric) & -target_amt)%>%
    summarise_each(~round(sum(.==0)/length(.)*100))%>%
    purrr::discard(select_col)%>%
    flextable()%>%
    set_caption("Percentage of Zeros in Numerical Covariates by Level of Target_Flag")

# Create statistical summaries for categorical variables at each level of target_flag 

target_0%>%
    diagnose_category()%>%
    flextable()%>%
    set_caption("Summary Statistics for Categorical Variables: Target Level = 0")

target_1%>%
    diagnose_category()%>%
    flextable()%>%
    set_caption("Summary Statistics for Categorical Variables: Target Level = 1")





```

We drop one observation where car_age is < 0 given that it is impossible to have
a negative age. This appears to be a data entry error given there are no other
negative values in this variable category. 

```{r}

# Drop observation with data error for car age (obs. value = -3). 

df$car_age[df$car_age < 0] <- NA


```

**1.4 Covariate Distributions**

Histograms and QQ-Plots indicate that distributions for target_flag, kidsdriv, homekids, yoj, income, oldclaim, clm_freq, and mvr_pts depart depart from a normal model. This is confirmed using a Shapiro-Wilk test of normality for numerical values.

Transformation of these variables may be desirable during model building. Log, square root, and polynomial transformations may be particularly helpful in this regard. 


```{r}

# Evaluate and rank departures from normality for numerical vars - Wilkes Shapiro

df %>%
    as.data.frame()%>%
    dplyr::select(-target_amt, target_flag)%>%
    normality() %>%
    filter(p_value <= 0.01) %>%
    arrange(abs(p_value))

#Identify skewness in covariates across levels of target_flag

target_0%>%find_skewness(index=FALSE, thres=TRUE) 
target_1%>%find_skewness(index=FALSE, thres=TRUE)

# Identify potential transformations to address skew, this does not resolve high 0 counts

df%>%
    dplyr::select(age, yoj, income, home_val, travtime, bluebook, tif, oldclaim, car_age)%>%
    plot_normality()

```
**1.5 Data Outliers**

The variables kidsrive, oldclaim, homekids, clm_freq, an yoj contain the highest percentage of outliers 
(~6-17%, depending on target_flag level). The percentage of outliers is generally greater for observations
associated with accidents (target_flag = 1) than without (target_flag = 0). 

The extent to which these/other outliers influence model parametrization is not yet know and should be
evaluated as part of the model building process.

```{r}

# Identify percentage of outliers for each covariate across levels of target_flag
    

diagnose_outlier(target_0) %>%
    filter(outliers_ratio > 5)%>%
    mutate(ratio_means= outliers_mean/with_mean)%>% # mean val of outliers/total mean
    arrange(desc(ratio_means))%>%
    dplyr::select(variables, outliers_ratio, ratio_means)%>%
    flextable()%>%
    set_caption("Outlier Summary: Target_Flag = 0")

diagnose_outlier(target_1) %>%
    filter(outliers_ratio > 5)%>%
    mutate(ratio_means= outliers_mean/with_mean)%>%
    arrange(desc(ratio_means))%>%
    dplyr::select(variables, outliers_ratio, ratio_means)%>%
    flextable()%>%
    set_caption("Outlier Summary: Target_Flag = 1")
```

**1.6 Missing Data**

Our dataset includes missing observations in one categorical variable (job) and six
numerical variables (car_age, home-val, yoj, income, age). Across covariates 
(i.e., predictor variables) the extent of missingness is < 6.5% of the total 
observations. There are no obvious patterns in the missing data that warrant concern.

```{r }

#Basic missing tables by target level
    
target_0%>%
    diagnose()%>%
    dplyr::select(-unique_count, -unique_rate)%>%
    filter(missing_count>0)%>%
    arrange(desc(missing_count))%>%
    flextable()%>%
    set_caption("Missing Data Summary: Target Level = 0")
    

target_1%>%
    diagnose()%>%
    dplyr::select(-unique_count, -unique_rate)%>%
    filter(missing_count>0)%>%
    arrange(desc(missing_count))%>%
    flextable()%>%
    set_caption("Missing Data Summary: Target Level = 1")
    

# Plot missing data to visualize cumulative percentages and col intersections

df %>% 
  plot_na_pareto(only_na = TRUE)

df %>%
    plot_na_intersect()  



```
**1.7 Impute Missing Values**

Given the relatively limited extent of missing data, an argument can be made for
imputing these values rather than removing the observations during model building. 
A variety of imputation methods are availabe for this purpose. They include:

- random selection of a value(s) between the minimum and maximum value of a covariate
- replacement with the mean, median, or mode of the covariate's distribution
- linear regression based on other covariates
- k-nearest neighbors
- recursive partitioning and regression trees
- multivariate imputation by chained equations (mice)

The following methods are selected for our model on the basis of initial experimentation:

- car age: mice
- home_val, yoj, income, age: recursive partitioning

Comparison of density plots for original and imputed values show a good fit 
for each of the numerical covariates.

```{r}

#Impute numerical vars using dlookr

#car_age

car_age<-imputate_na(df, car_age, target_flag, method = "mice", seed = 999)

p1<-plot(car_age)+theme_minimal()+theme(legend.position = "top")

summary(car_age)


#home_val


home_val<-imputate_na(df, home_val, target_flag, method = "rpart")

p2<-plot(home_val)+theme_minimal()+theme(legend.position = "top")

summary(home_val)

#yoj


yoj<- imputate_na(df, yoj, target_flag, method = "rpart")

p3<-plot(yoj)+theme_minimal()+theme(legend.position = "top")

summary(yoj)

# income

income<-imputate_na(df, income, target_flag, method = "rpart")

p4<-plot(income)+theme_minimal()+theme(legend.position = "top")

summary(income)

#age

age<-imputate_na(df, age, method = "rpart")

p5<-plot(age)+theme_minimal()+theme(legend.position = "top")

summary(age)

temp<-cbind(car_age, home_val, yoj, income, age)
temp%<>%as.data.frame(temp)

df%<>%dplyr::select(!c(car_age, home_val, yoj, income, age))%>%
    cbind(temp)

df%<>%mutate_if(is.numeric, round)

# print plots

(p1|p2)/(p3|p4)/p5


```
We can also impute missing values for categorical variables. In this case, the 
'mice' method is used to impute values for the variable, job. 

```{r}

job<-imputate_na(df, job, method = "mice", seed = 999)

plot(job)+theme_minimal()+theme(legend.position = "top")


# combine into new dfb

df<-df%>%
    dplyr::select(!job)

df<-cbind(df,job)

df$job<-factor(df$job)

```
Save csv with imputed values 

```{r}

#df%<>%mutate_at(vars(target_flag), funs(factor)) # convert target_flag to factor

#write.csv(df, 'df_clean.csv', row.names=FALSE)

```

**1.8 Pairwise Comparisons**

Pairwise comparisons provide a method to identify collinear relationships between
covariates. Covariates that display high collinearity (e.g., correlation coefficients
< -.50  or > .50) can reduce the precision and increase the sensitivity of model 
coefficients due to lack of independence.     

To refine our analyses, pairwise comparisons are assessed following removal of 
0 values. Our results indicate:

- high positive correlation between home_val and income, coeff ~ 0.96
- high positive correlation between homekids and homedrive, coeff ~ 0.62
- moderate positive correlation between home_val and car_age, coeff ~ 0.54
- moderate positive correlation between income and car_age, coeff ~ 0.54

As expected, there is also a strong association between income (numerical var.) 
and job type (factor var.) - based on a kruskal-Wallis rank sum test.

Both home_value and income have similar levels of missing data. An argument can be
made for dropping either home-value or income from the dataset in order to simplify 
our feature selection process and improve model precision. This may also be
true for income and job type.

Associations between home_val and car_age and income and car_age are less 
concerning, due to lower correlation coefficients and higher variability. See
plots below.

Less clear is the association between homekids and kidsdriv (ordinal vars.)
given their narrow range of values. 

We will revisit options for feature removal during the model building process.


```{r}

#Assess collinearity in the absence of zero values in numerical cols

df%>%
    filter_if(is.numeric, all_vars((.) != 0))%>%
    correlate()%>%
    filter(coef_corr > .5 | coef_corr < -.5)%>% # set thresholds to limit output 
    flextable()%>%
    set_caption("Pairwise Correlation: Numerical Covariates")

# Plot relationship between numerical covariates (w/o zero values for clarity) beyond the threshold coef_corr 

df %>%
    filter(income != 0 & home_val != 0)%>% 
    target_by(income) %>%
    relate(home_val) %>%
    plot()
    

df %>%
    filter(homekids != 0 & kidsdriv != 0)%>%
    target_by(homekids) %>%
    relate(kidsdriv) %>%
    plot()

df %>%
    filter(home_val != 0 & car_age != 0)%>%
    target_by(home_val) %>%
    relate(car_age) %>%
    plot()

df %>%
    filter(income != 0 & car_age != 0)%>%
    target_by(income) %>%
    relate(car_age) %>%
    plot()


# Plot relationship between numerical and factors for select variables

df %>% 
  target_by(income) %>%      
  relate(job) %>% 
  plot()

df%>%kruskal.test(income~job) # assess covariance with Kruskal-Wallis rank sum test where > 1 categorical level


```
We can visually assess relationships between categorical variables and 
our response variable, target-flag, using mosaic plots. In this case,
a relationship is indicated by relative differences in variable 
levels for each level of target-flag.

Note that the width of plot columns is proportional to the count of observations
for each level of the variable plotted on the horizontal axis. And the vertical
lenght is proportional to the count in the second variable (i.e., target_flag)
within each level of the first variable. 

On this basis we find that:

- All categorical variables except 'sex' and 'red-car' display relationships 
to target_flag.

An argument can be made for dropping 'sex and 'red_car' from our set of covariates
in order to streamline the modeling process. 

```{r }

# Subset categorical variables into a df of factor vars

mosaic<-df%>% #subset target_flag as factor
    mutate_at(vars(target_flag), funs(as.factor))%>%
    dplyr::select(where(is.factor))

target <- all_of(names(mosaic)[1]) # set name for target_flag
predictors <- all_of(names(mosaic)[2:11])

#generate mosaic plots with purrr and dlookr

(plots<-predictors%>%map(function(predictors) mosaic%>%target_by(target)%>%relate(predictors)%>%plot()))




```
Boxplots make an effective visual tool to assess relationships between target_flag
and our numerical covariates. 

On this basis, it appears that 'bluebook', 'tif', 'clm_freq', 'mvr_pts', 'car_age',
'home_val', and 'income' vary by level of target_flag. The presence of outliers,
however, argues for caution in this interpretation. 


```{r}

# Subset numerical variables

num_box<-select_if(df, is.numeric)%>%
    mutate_at(vars(target_flag), funs(as.factor))%>%
    dplyr::select(-target_amt)

# Plot using boxplots

response = names(num_box)[1] #target_flag (needs to be fct for these plots)
response = purrr::set_names(response)

explain <- names(num_box)[2:14] #explanatory variables
explain = purrr::set_names(explain)

box_fun = function(x) {
    ggplot(num_box, aes_string(x = x, y = 'target_flag') ) +
    geom_boxplot(aes(fill = target_flag, alpha = 0.4), outlier.color =
    'red', show.legend = FALSE)+
    scale_fill_viridis(discrete = TRUE, option = "E")+
    coord_flip()+
    theme_classic()
}

box_plots<-map(explain, ~box_fun(.x)) #creates a list of plots using purrr

plot(reduce(box_plots,`+`)) # layout with patchwork
```
**1.9 Final Variable Selections**

Before forming training and testing datasets, we will drop 'home_val' and 
'target_amt'. Variability in the model explained by 'home_val' will be largely
explaine by 'income'. And 'target_amt'  is not relevant to our classification 
process.

We will also collapse the variable 'job' into two levels: 'non-white-collar' and 
'white-collar'. While this may result in a slight loss of discriminatory power in 
our model(s) it does lend to model simplification.

Rather than make an other apriori choices to drop other covariates, we will 
automate feature selection as part of the modeling process.  


```{r}

base_df<-df%>%
    dplyr::select(-home_val, -target_amt)  # variance in these fields explained by income

base_df%<>%
    mutate(job = (fct_collapse(job,
                 not_white_collar = c("Student", "Blue Collar", "Home Maker", "Clerical", "Manager"),
                 white_collar = c("Doctor", "Lawyer", "Professional"))))

#write.csv(base_df, 'base_df.csv', row.names=FALSE)    
```

##2. Training and Test Datasets##

use caret - rebalancing train for model building, test maintains relative freqs

```{r}
# Base_df<-read.csv('base_df.csv') # hold here to assist model dev 

base_df%>%count(target_flag)%>%
    mutate(frequency = n/sum(n))%>%
    flextable()%>%  #--- > 0 = 6008, 1 = 2153
    set_caption("Count and Frequency of Target Variable by Level")

ones <- base_df%>%
    filter(target_flag == "1")

zeros <-base_df%>%
    filter(target_flag =="0")

set.seed(1234) 

# Build train/test sets; down-sample to re-balance test dataset

input_ones <- sample(1:nrow(ones), 0.7*nrow(ones))
input_zeros <- sample(1:nrow(zeros), 0.7*nrow(ones))

train_ones <- ones[input_ones, ]
train_zeros <- zeros[input_zeros, ]

train_df<-rbind(train_ones, train_zeros)

test_ones <- ones[-input_ones,]
test_zeros <- zeros[-input_zeros,]

test_df <- rbind(test_ones, test_zeros)

#Check counts and frequencies of target_flag for train set and test set 

train_df%>%count(target_flag)%>%
    mutate(frequency = n/sum(n))%>%
    flextable()%>%
    set_caption("Count and Frequency of Target Variable: Training Data")

test_df%>%count(target_flag)%>%
    mutate(frequency = n/sum(n))%>%
    flextable()%>%
    set_caption("Count and Frequency of Target Variable: Test Data")

```

##3. Build Models##

Some description here

**3.1 Null Model: Logistic Regression**

```{r}

null<-glm(target_flag ~ 1, family = binomial, train_df)

summary(null)

null_stats<-glance(null) # collect stats for model comparisons

```

**3.2 Full Model: Logistic Regression**

```{r}

full<-glm(target_flag ~ ., family = binomial, train_df) 

summary(full)


full_stats<-glance(full) # collect stat metrics for model comparisons

```
**3.3 Reduced Model: Logistic Regression **

```{r}

#Build reduced model

reduced <- glm(target_flag ~ .,family='binomial', train_df) 

reduced<-step(reduced, trace=0) # use Akiaike step for covariate selection, trace 0 prevents intermediate printing

summary(reduced)

reduced_stats<-glance(reduced) # collect stats for model comparisons

```
**3.3.1 Diagnostics: Reduced Model**

The ROC curve shows the tradeoff between true positive rate and false positive rate across all cutoff values.

Note: InformationValue:confusionMatrix -- wants predicts and obs to be numerical, in caret they should be factor!

```{r}

# Collect predicted percentages

predicted<-predict(reduced,train_df,type='response')

# Find optimal threshold to minimize misclassification

optimal<- optimalCutoff(train_df$target_flag, predicted, optimiseFor = "misclasserror")  # 0.51

# Calculate training classification error with .51 threshold --> 0.2595

Miss_Class_Error<-misClassError(train_df$target_flag, predicted,  threshold = optimal)

# Convert predicted to 0, 1 values based on threshold

predicts<-ifelse(predicted >= optimal, 1, 0)

# Subset actuals and predicts as numeric for confusion matrix

actuals<-as.numeric(as.character(train_df$target_flag))

(InformationValue::confusionMatrix(actuals, predicts, threshold = optimal)) 

#Calculate related metrics

AUC<-AUROC(actuals, predicted) # -->.81

Sensitivity<-InformationValue::sensitivity(actuals, predicts, threshold = optimal) # recall: true positive rate
Specificity<-InformationValue::specificity(actuals, predicts, threshold = optimal) #false positive rate
Precision<-InformationValue::precision(actuals, predicts, threshold = optimal) # prop of predicted ones /prop total ones

YDI_Index<- youdensIndex(actuals, predicts, threshold = optimal) # 0.48 accounts for both false-positive and false-negative rates

# Assess predictive power using concordance

Predict_Power<-Concordance(actuals, predicts) # predictive power  ---> .548
Predict_Power<-Predict_Power$Concordance

ks_plot(actuals, predicts) #cum perc of ones captured by the model against that expected random

Kol_Smirnov<-ks_stat(actuals, predicts) #Kolmogorov-Smirnov statistic - maximum difference between the cumulative true positive and cumulative false positive rate

# Create a metrics table

reduced_metrics<-cbind(Predict_Power, Miss_Class_Error, AUC, Sensitivity, Specificity, Precision, YDI_Index, Kol_Smirnov)

# Construct ROC curve: note actuals, predicts, predicted as vectors)

pred <- ROCR::prediction(predicted, actuals)
perf <- ROCR::performance(pred, "tpr", "fpr")

plot(perf, colorize = TRUE, main = "ROC curve for logistic regression on insurance data")

#AUC curve

plotROC(actuals, predicted)


```

**3.3.2 Reduced Model: Overdispersion**

- we are within tolerable levels of dispersion

```{r}

# Evaluate using deviance and quasibinomial comparison

deviance(reduced)/df.residual(reduced) # if considerably greater than one we should be concerned

# Check with two model fit

quasi_model <-  glm(target_flag ~ .,family='quasibinomial', base_df) # note: using base_df
  
pchisq(summary(quasi_model)$dispersion * reduced$df.residual,
 reduced$df.residual, lower = F)  
```
**3.3.3 Reduced Model: Check linearity of numerical vars vs. logit***

Linearity is questionable for yoj, oldclaim

Note clustering in oldclaim - consider turning into 3-level factor?

```{r}

#create a dataframe for linearity analysis

reduced_df<-train_df

reduced_df$predicted<-predict(reduced,train_df,type='response') # add predicted

reduced_df%<>%mutate(logit = log(predicted/(1-predicted))) # add logit


par(mfrow = c(2,4)) 

# check linearity btwn numerical predictors and logit

with(reduced_df, scatter.smooth(travtime, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(reduced_df, scatter.smooth(tif, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(reduced_df, scatter.smooth(mvr_pts, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(reduced_df, scatter.smooth(income, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(reduced_df, scatter.smooth(homekids, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(reduced_df, scatter.smooth(oldclaim, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(reduced_df, scatter.smooth(clm_freq, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(reduced_df, scatter.smooth(bluebook, logit, lpars = list(col = "red", lwd = 3, lty = 3)))



```

**3.3.4 Reduced Model: Outliers and Infuential obs**

computes the standardized residuals (.std.resid) and the Cookâ€™s distance (.cooksd) using the R function augment() [broom package]

See: http://www.sthda.com/english/articles/36-classification-methods-essentials/148-logistic-regression-assumptions-and-diagnostics-in-r/

Findings:

Cooks distance indicates several standout obs (3722, 3592, 6501) but no influential points (id. D >1.0)

There are no obs has std residual beyond 3 stdev: 5101


```{r}

# Extract model results

reduced_df$data <- augment(reduced) %>% #add model output to dataframe
  mutate(index = 1:n()) 

# Identify Outliers

reduced_df$data %>% top_n(10, .cooksd)

plot(reduced, which = 4, id.n = 3)  # keep an eye on obs > 4/n 

#Plot Standardized Residuals

ggplot(reduced_df$data, aes(index, .std.resid)) + 
  geom_point(aes(color = target_flag), alpha = .5) +
  theme_bw()

#Filter potential influential data points with abs(.std.res) > 3

reduced_df$data %>% 
  filter(abs(.std.resid) > 3)


```
**3c.3.5. Reduced Model: Collinearity**

Findings: no problems with collinearity

```{r}

car::vif(reduced)
```

**3.3.6. Reduced Model: Independence of Residuals**

We see some possible pattern (curvature) that might indicate misclassification but its unclear 

```{r }

resid_df<-train_df%>%mutate('residuals' = residuals(reduced), linpred = predict(reduced))

bins<- group_by(resid_df, cut(linpred, breaks=unique(quantile(linpred, (1:100)/101))))

diag<-summarize(bins, residuals=mean(residuals), linpred = mean(linpred))

plot(residuals~linpred, diag, xlab = "linear predictor")


```

**3.3.7 Reduced Model: Goodness of Fit - Marginal Plots**

```{r}
marginals_reduced<-mmps(reduced,main=NULL)
```

**3.4 Transformation Model**

```{r fig.align='center', fig.height=10,fig.width=10, message = FALSE}


#Build model with transformed vars

trans <- glm( target_flag ~ kidsdriv + parent1 + mstatus + education + 
    log(travtime) + car_use + log(bluebook) + sqrt(tif) + car_type + (oldclaim) + 
    clm_freq + revoked + mvr_pts + urbanicity + sqrt(income) + age, family='binomial', train_df)

trans<-step(trans, trace=0) 

summary(trans)

trans_stats<-glance(trans) # save stats for model comparisons


```
**3.4.1 Transformation Model: Diagnostics **

```{r}

# collect predicted percentages

trans_predicted<-predict(trans, train_df,type='response')

# find optimal threshold to minimize misclassification

trans_optimal<- optimalCutoff(train_df$target_flag, trans_predicted, optimiseFor = "misclasserror")  # 0.51

# calculate training classification error with .51 threshold --> 0.2588

Miss_Class_Error<-misClassError(train_df$target_flag, trans_predicted,  threshold = trans_optimal)

# convert predicted to 0, 1 values based on threshold

trans_predicts<-ifelse(trans_predicted >= trans_optimal, 1, 0)

# subset actuals and predicts as numeric for confusion matrix

trans_actuals<-as.numeric(as.character(train_df$target_flag))

InformationValue::confusionMatrix(trans_actuals, trans_predicts, threshold = optimal)%>%
    flextable()%>%
    set_caption("Confusion Matrix: Reduced Model")

#Calculate related metrics

AUC<-AUROC(trans_actuals, trans_predicted) # -->.81
Sensitivity<-InformationValue::sensitivity(trans_actuals, trans_predicts, threshold = optimal) # recall: true positive rate
Specificity<-InformationValue::specificity(trans_actuals, trans_predicts, threshold = optimal) #false positive rate
Precision<-InformationValue::precision(trans_actuals, trans_predicts, threshold = optimal) # prop of predicted ones /prop total ones
YDI_Index<-youdensIndex(trans_actuals, trans_predicts, threshold = optimal) # 0.48 accounts for both false-positive and false-negative rates

# Calculate predictive power using concordance

Concord<-Concordance(trans_actuals, trans_predicts)
Predict_Power<-Concord$Concordance

ks_plot(trans_actuals, trans_predicts) #cum perc of ones captured by the model against that expected random

Kol_Smirnov<-ks_stat(trans_actuals, trans_predicts) #Kolmogorov-Smirnov statistic - maximum difference between the cumulative true positive and cumulative false positive rate

# Create a metrics table

trans_metrics<-cbind(Predict_Power, Miss_Class_Error, AUC, Sensitivity, Specificity, Precision, YDI_Index, Kol_Smirnov)

# Construct ROC curve: note actuals, predicts, predicted as vectors)

pred_trans <- ROCR::prediction(trans_predicted, trans_actuals)
perf_trans <- ROCR::performance(pred_trans, "tpr", "fpr")

plot(perf_trans, colorize = TRUE, main = "ROC curve for logistic regression on insurance data")

#AUC curve

plotROC(trans_actuals, trans_predicted)
```
**3.4.2 Transformed Model: Goodness of Fit **

```{r}
marginals_trans<-mmps(trans,main=NULL)
```


##4. Model Selection##

Note - trans - marginals similar to reduced, collinearity similar, linearity similar

```{r}

# Compare model output summaries

col_name<-c('Null Model', 'Full Model', 'Reduced Model', 'Transformed Model')

comp<-rbind(null_stats, full_stats, reduced_stats, trans_stats)

(compare_models<- cbind(col_name, comp)%>%
    dplyr::select(-c(null.deviance, df.null))%>%
    rename(Model = col_name)%>%
    flextable()%>%
     set_caption("Comparison of Model Results"))

# Compare select metrics for reduced and transformed models

col_final<-c('Reduced Model', 'Transformed Model')

final_comp<-rbind(reduced_metrics, trans_metrics)

(final_models<- cbind(col_final, final_comp)%>%
    as.data.frame()%>%
    mutate_at(vars(2:9), funs(as.numeric))%>%
    mutate_if(is.numeric, round, 3)%>%
    rename(Model = col_final)%>%
    flextable()%>%
    set_caption("Classification Metrics: Reduced & Transformed Models"))
```

##5. Model Evaluation##

```{r}

# Apply reduced model to test data

reduced_test<-glm(formula = target_flag ~ kidsdriv + parent1 + mstatus + 
    education+travtime + car_use + bluebook + tif + car_type + oldclaim + 
    clm_freq + revoked + mvr_pts + urbanicity + income + age, 
    family = "binomial", data = test_df)

# Apply fitted model to test sample (predicted probabilities)

predicted_test <- predict(reduced_test, test_df, type="response")

# Find optimal threshold to minimize misclassification

test_optimal<- optimalCutoff(test_df$target_flag, predicted_test, optimiseFor = "Both") # --> .149

# Calculate training classification error with .51 threshold

Miss_Class_Error<-misClassError(test_df$target_flag, predicted_test,  threshold = test_optimal)

# Convert predicted to 0, 1 values based on threshold

predicts_test<-ifelse(predicted_test >= test_optimal, 1, 0)

# Cubset actuals and predicts as numeric for confusion matrix

actuals_test<-as.numeric(as.character(test_df$target_flag))

InformationValue::confusionMatrix(actuals_test, predicts_test, threshold = test_optimal)%>%
    flextable()%>%
    set_caption("Confusion Matrix: Transformation Model")

#Calculate related metrics

AUC<-AUROC(actuals_test, predicted_test) # -->.81
Sensitivity<-InformationValue::sensitivity(actuals_test, predicts_test, threshold = .17) # recall: true positive rate
Specificity<-InformationValue::specificity(actuals_test, predicts_test, threshold = .17) #false positive rate
Precision<-InformationValue::precision(actuals_test, predicts_test, threshold = .17) # prop of predicted ones /prop total ones
YDI_Index<-youdensIndex(actuals_test, predicts_test, threshold = test_optimal) # 0.48 accounts for both false-positive and false-negative rates

# Calculate predictive power using concordance

Concord<-Concordance(actuals_test, predicts_test)
Predict_Power<-Concord$Concordance

ks_plot(actuals_test, predicts_test) #cum perc of ones captured by the model against that expected random

Kol_Smirnov<-ks_stat(actuals_test, predicts_test) #Kolmogorov-Smirnov statistic - maximum difference between the cumulative true positive and cumulative false positive rate

# Create a metrics table

(test_metrics<-cbind(Predict_Power, Miss_Class_Error, AUC, Sensitivity, Specificity, Precision, YDI_Index, Kol_Smirnov)%>%
    as.data.frame()%>%
    mutate_at(vars(1:8), funs(as.numeric))%>%
    mutate_if(is.numeric, round, 3)%>%
    flextable()%>%
    set_caption("Reduced Model-Test Data: Metrics"))
    

# Construct ROC curve: note actuals, predicts, predicted as vectors)

pred <- ROCR::prediction(predicted_test, actuals_test)
perf <- ROCR::performance(pred, "tpr", "fpr")

plot(perf, colorize = TRUE, main = "ROC curve for logistic regression on insurance data")

#Construct AUC curve

plotROC(actuals_test, predicted_test)

```

##6. Summary##

To be completed